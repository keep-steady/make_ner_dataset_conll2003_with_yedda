{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make ner dataset CoNLL2003 format with ann file from yedda annotation tool\n",
    "\n",
    "191018\n",
    "\n",
    "BIOES format\n",
    "\n",
    "Aotomate chunking with NLTK\n",
    "\n",
    "\n",
    "Get CoNLL2003 format using yedda code(to make anns file)\n",
    "\n",
    "However, sometimes there is critical errors because the difference between NLTK chunking tokenize and tagging token tokenize\n",
    "\n",
    "We try 2 ways to get the CoNLL2003 format from ann file(yedda annotation tool result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put your input .ann folder \n",
    "\n",
    "folder_path = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add space right after *] \n",
    "# add space right before [@\n",
    "def _split(pattern):\n",
    "    string = str(pattern.group())    \n",
    "    if '[@' in string:\n",
    "        return string.split('[')[0] + ' ' + '[@'\n",
    "    elif '[$' in string:\n",
    "        return string.split('[')[0] + ' ' + '[$'\n",
    "    elif '*]' in string:\n",
    "        return '*]' + ' ' + string.split(']')[1]\n",
    "\n",
    "\n",
    "# add space if there is '.' in patter\n",
    "def _split_comma(pattern):\n",
    "    string = str(pattern.group())\n",
    "    if '.' in string:\n",
    "        return string.split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 2 ea\n",
      "Done, error : 0\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir(folder_path)\n",
    "print('doc %d ea' % (len(file_list)))\n",
    "# ~_conll folder\n",
    "conll_folder_path = folder_path + '_conll'\n",
    "if not os.path.isdir(conll_folder_path): os.mkdir(conll_folder_path)\n",
    "\n",
    "    \n",
    "error_sentence_list = []  # save error sentence\n",
    "for file_path in file_list:\n",
    "#     filename = 'test.ann'\n",
    "    \n",
    "    ## 1. load\n",
    "    sentences = []\n",
    "    with open(os.path.join(folder_path, file_path), 'r', encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            sentences.append(line)\n",
    "        \n",
    "    conll_document = []\n",
    "    ## 2. line running\n",
    "    for sentence in sentences:\n",
    "        conll_sentence_with_chunking = []\n",
    "        sentence = sentence.replace(\" '\", \" ' \")\n",
    "        \n",
    "        if sentence != '':\n",
    "            ##############################################################################\n",
    "            ## 1) data preprocessing\n",
    "            ##############################################################################\n",
    "            ## 1. '.' tokenize problem, split before&after '.'\n",
    "            # 1.1. find patterns\n",
    "            pattern_list = re.findall(r'\\[[\\@\\$)].*?\\#.*?\\*\\](?!\\#)', sentence)\n",
    "            for pattern in pattern_list:\n",
    "                if '.' in pattern:\n",
    "                    no_comma_pattern = pattern.replace('.', ' . ')\n",
    "            # 1.2. if there is '.', add space before&after\n",
    "                    sentence = sentence.replace(pattern, no_comma_pattern)\n",
    "            # refind patterns\n",
    "            pattern_list = re.findall(r'\\[[\\@\\$)].*?\\#.*?\\*\\](?!\\#)', sentence)\n",
    "            for pattern in pattern_list:\n",
    "                if '[$' in pattern:\n",
    "                    no_comma_pattern = pattern.replace('[$', '[@')\n",
    "                    sentence = sentence.replace(pattern, no_comma_pattern)      \n",
    "            ##############################################################################\n",
    "            ## 2. tokenize problen solving\n",
    "            ## 2.1. add space after tag('*]'') if no space agter\n",
    "            taged_sentence_back     = re.sub(r'\\*\\][^\\ ]', _split, sentence)\n",
    "            ## 2.2. add space at before tag('[@' or '[$') if no space before tag\n",
    "            taged_sentence_filtered = re.sub(r'[^\\ ]\\[[\\@\\$]', _split, taged_sentence_back)            \n",
    "            ##############################################################################\n",
    "            \n",
    "            \n",
    "            ##############################################################################\n",
    "            ## 2) conll tagging\n",
    "            ##############################################################################\n",
    "            ## 1. make conll tagging format \n",
    "            conll_format = make_ann2conll(taged_sentence_filtered)\n",
    "            ##############################################################################\n",
    "            ## 2. make chunk format\n",
    "            ## 2.1 restore sentence\n",
    "            \n",
    "            pure_line = re.sub(r'\\[[\\@\\$)].*?\\#.*?\\*\\](?!\\#)', back2pure_line, taged_sentence_filtered)\n",
    "            ## 2.2 make chunk\n",
    "            chunking     = make_chunk(pure_line)\n",
    "            ##############################################################################\n",
    "            ## 3. check the the length between chunking and tagging is different or not\n",
    "            ## 3.1. if the lengh is same, merge chunk and tagging\n",
    "            if len(chunking) == len(conll_format):\n",
    "                \n",
    "                for i in range(len(chunking)):\n",
    "                    conll_sentence_with_chunking.append(chunking[i] + [conll_format[i][-1]])\n",
    "                # add black after sentence\n",
    "                conll_sentence_with_chunking.append([''])\n",
    "            \n",
    "            ## 3.2. If the length between chunking and tagging is different\n",
    "            else:\n",
    "                try:\n",
    "                    if len(chunking) != len(conll_format):\n",
    "                        print('!!! - not possible with first way')\n",
    "\n",
    "                        conll_format1 = make_ann2conll(sentence)\n",
    "                        conll_format2 = get_tagging_token(sentence)\n",
    "                        ##############################################################################\n",
    "                        ## 2. make chunk format\n",
    "                        ## 2.1 restore sentence\n",
    "                        pure_line = make_pure_sentence_from_tagged(sentence)\n",
    "                        ## 2.2 make chunk\n",
    "                        chunking     = make_chunk(pure_line)\n",
    "                        print('chunk : %d / conll1 : %d / conll1 : %d'%(len(chunking), len(conll_format1), len(conll_format2)))                    \n",
    "\n",
    "                        if len(chunking) != len(conll_format2):\n",
    "                            print('!!! - not possible with second way, Fix ann file with hand....')\n",
    "                            error_sentence_list.append([file_path, sentence, pure_line, chunking, conll_format, len(chunking), len(conll_format)])\n",
    "                        else:\n",
    "                            print('!!!!! - second way, complete')\n",
    "                            conll_format = conll_format2\n",
    "                            # complete conversion, merge\n",
    "                            for i in range(len(chunking)):\n",
    "                                conll_sentence_with_chunking.append(chunking[i] + [conll_format[i][-1]])\n",
    "                            # add black after sentence\n",
    "                            conll_sentence_with_chunking.append([''])\n",
    "                except:\n",
    "                    print('Fix ann file with hand....')\n",
    "                    error_sentence_list.append([file_path, sentence, pure_line, chunking, conll_format, len(chunking), len(conll_format)])\n",
    "\n",
    "        # add doc_list after sentence end\n",
    "        conll_document.append(conll_sentence_with_chunking)\n",
    "    \n",
    "    ####################################################################\n",
    "    ## save\n",
    "    ####################################################################\n",
    "    ## save as document unit\n",
    "    fila_name = file_path.split('.ann')[0]\n",
    "    fw = open(os.path.join(conll_folder_path, fila_name  + '.conll'), 'w')\n",
    "    # start document with file name\n",
    "    fw.write('<<doc>> ' + fila_name +'\\n\\n')\n",
    "    for conll_line in conll_document:\n",
    "        for conll_token in conll_line:\n",
    "            if len(conll_token) > 0:\n",
    "                try:\n",
    "                    fw.write('\\t'.join(conll_token) + '\\n')\n",
    "                except:\n",
    "                    pass\n",
    "    fw.close()\n",
    "    ####################################################################\n",
    "    ####################################################################\n",
    "print('Done, error : %d'%(len(error_sentence_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(error_sentence_list))\n",
    "error_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No error!!\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "get_error_information(error_sentence_list, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
